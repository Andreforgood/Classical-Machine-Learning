# Define the negative log-likelihood function for optimization
def negative_log_likelihood(params):
    """ Calculate the negative log likelihood for the Gaussian process """
    # Unpack parameters
    length_scale, alpha = params
    
    # Calculate the kernel matrix for the training data
    K = rational_quadratic_kernel(X, X, length_scale, alpha)
    K_y = K + np.eye(len(X)) / beta  #This is the matrix C
    
    # Calculate the log likelihood
    L = np.linalg.cholesky(K_y)
    log_det = 2 * np.sum(np.log(np.diag(L)))  # log determinant of K_y
    inv_y = np.linalg.solve(L.T, np.linalg.solve(L, Y)) # use of the solution for a Systems of Linear Equations 
    
    return 0.5 * (Y.T.dot(inv_y) + log_det + len(X) * np.log(2 * np.pi))


# Initial guess for the parameters
initial_guess = [1.0, 1.0]

# Use scipy's minimize to optimize the negative log likelihood
#result = minimize(negative_log_likelihood, initial_guess, bounds=((1e-5, None), (1e-5, None)))
result = minimize(negative_log_likelihood, initial_guess)

# Optimized parameters
optimized_params = {'length_scale': result.x[0], 'alpha': result.x[1]}
print("Optimized parameters:", optimized_params)

# Predict using optimized parameters
mu_optimized, std_optimized = gaussian_process_regression(X, Y, X_pred, rational_quadratic_kernel, optimized_params)

# Visualization of optimized results
plt.figure(figsize=(10, 6))
plt.scatter(X, Y, c='red', label='Training data')
plt.plot(X_pred, mu_optimized, 'b-', label='Mean of f (optimized)')
plt.fill_between(X_pred.flatten(), mu_optimized-1.96*std_optimized, mu_optimized+1.96*std_optimized, alpha=0.2, color='blue', label='95% Confidence Interval (optimized)')
plt.title('Optimized Gaussian Process Regression')
plt.xlabel('X')
plt.ylabel('f(X)')
plt.legend()
plt.show()
